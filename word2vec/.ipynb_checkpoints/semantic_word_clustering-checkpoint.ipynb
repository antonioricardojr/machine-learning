{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It loads the dataframe, gets the column, joins all the values, removes the punctuation and split into empty spaces.\n",
    "df = pd.read_csv(\"../data/casos.csv\")\n",
    "summaries = \"\".join(l for l in df['DS_RESUMO'].str.cat(sep=', ') if l not in string.punctuation)\n",
    "words = summaries.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 110280\n",
      "unique words: 16603\n"
     ]
    }
   ],
   "source": [
    "words_freq = collections.Counter(words).most_common()\n",
    "print(\"words: {0}\".format(len(words)))\n",
    "print(\"unique words: {0}\".format(len(words_freq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de (8,603)\n",
      "do (2,542)\n",
      "DE (2,048)\n",
      "da (1,843)\n",
      "a (1,740)\n",
      "em (1,694)\n",
      "e (1,425)\n",
      "no (1,347)\n",
      "o (1,204)\n",
      "nº (969)\n",
      "Apurar (940)\n",
      "na (894)\n",
      "que (881)\n",
      "por (828)\n",
      "DA (756)\n",
      "crime (751)\n",
      "com (659)\n",
      "para (629)\n",
      "ao (553)\n",
      "dos (489)\n",
      "\n",
      "Least common:\n",
      "\n",
      "sito (1)\n",
      "má (1)\n",
      "Dom (1)\n",
      "Notitia (1)\n",
      "226228 (1)\n",
      "CARMEM (1)\n",
      "ISANA (1)\n",
      "noticiam (1)\n",
      "Amanda (1)\n",
      "27062006 (1)\n",
      "28022011 (1)\n",
      "RETRATANDO (1)\n",
      "Remanso (1)\n",
      "IKEZIRI (1)\n",
      "128000001117201640 (1)\n",
      "320763968811 (1)\n",
      "DELZUITA (1)\n",
      "indicio (1)\n",
      "SUBSCREVEU (1)\n",
      "128000001242201731 (1)\n"
     ]
    }
   ],
   "source": [
    "for word, freq in words_freq[:20]:\n",
    "    print('{} ({:,d})'.format(word, freq))\n",
    "print('\\nLeast common:\\n')\n",
    "for word, freq in words_freq[-20:]:\n",
    "    print('{} ({:,d})'.format(word, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words 5+: 2,584\n"
     ]
    }
   ],
   "source": [
    "words_5plus = sum(1 for _, freq in words_freq if freq >= 5)\n",
    "\n",
    "print('Words 5+: {:,d}'.format(words_5plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Marinha', 4)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 2600\n",
    "\n",
    "words_freq[vocabulary_size - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for the vocabulary: 2,599\n"
     ]
    }
   ],
   "source": [
    "words_vocab = words_freq[:(vocabulary_size-1)]\n",
    "\n",
    "print('Words for the vocabulary: {:,d}'.format(len(words_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2600\n"
     ]
    }
   ],
   "source": [
    "UNK_ID = 0\n",
    "word_to_id = dict((word, word_id) for word_id, (word, _) in enumerate(words_vocab, UNK_ID+1))\n",
    "word_to_id['UNK'] = UNK_ID\n",
    "word_from_id = dict((word_id, word) for word, word_id in word_to_id.items())\n",
    "\n",
    "print('Vocabulary size: {:d}'.format(len(word_to_id)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "words_to_unk = words_freq[(vocabulary_size-1):]\n",
    "unk_freq = sum(freq for _, freq in words_to_unk)\n",
    "\n",
    "print('UNK words: {:,d}'.format(len(words_to_unk)))\n",
    "print('UNK frequency: {:,d}'.format(unk_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file size: 21,968 bytes\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_FILE = '../data/vocabulary.txt'\n",
    "\n",
    "with open(VOCABULARY_FILE, 'w') as f:\n",
    "    for word_id in range(vocabulary_size):\n",
    "        f.write(word_from_id[word_id] + '\\n')\n",
    "\n",
    "print('Vocabulary file size: {:,d} bytes'.format(os.stat(VOCABULARY_FILE).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3,000\n"
     ]
    }
   ],
   "source": [
    "with open(VOCABULARY_FILE, newline='') as f:\n",
    "    word_from_id_ = dict((word_id, word.strip()) for word_id, word in enumerate(f))\n",
    "    word_to_id_ = dict((word, word_id) for word_id, word in word_from_id_.items())\n",
    "\n",
    "# print(word_from_id_)\n",
    "print('Vocabulary size: {:,d}'.format(len(word_to_id_)))\n",
    "assert word_to_id_ == word_to_id\n",
    "assert word_from_id_ == word_from_id\n",
    "del word_to_id_, word_from_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "\n",
      "110,280\n",
      "\n",
      "Text (IDs):\n",
      "\n",
      "[0, 729, 15, 22, 72, 399, 959, 1, 368, 13]\n",
      "\n",
      "Text (Words):\n",
      "\n",
      "['UNK', 'GONÇALVES', 'DA', 'SILVA', 'teria', 'apresentado', 'comprovante', 'de', 'residência', 'que']\n"
     ]
    }
   ],
   "source": [
    "data = list(word_to_id.get(word, UNK_ID) for word in words)\n",
    "\n",
    "print('Size:\\n\\n{:,d}\\n'.format(len(data)))\n",
    "print('Text (IDs):\\n\\n{}\\n'.format(data[:10]))\n",
    "print('Text (Words):\\n\\n{}'.format(list(word_from_id[word_id] for word_id in data[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110280"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context_window(window_words, target_index):\n",
    "    '''This function returns the words at the window without the target word.'''\n",
    "    words = list(window_words)\n",
    "    del words[target_index]\n",
    "    return words\n",
    "\n",
    "def input_cbow(data, batch_size, window_size):\n",
    "    if window_size % 2 == 0 or window_size < 3 \\\n",
    "        or window_size > (len(data) - batch_size) / 2:\n",
    "        # {window_size} must be odd: (n words left) target (n words right)\n",
    "        raise Exception(\n",
    "            'Invalid parameters: window_size must be a small odd number')\n",
    "\n",
    "    num_words = len(data)\n",
    "    num_windows = num_words - window_size + 1\n",
    "    num_batches = num_windows // batch_size\n",
    "    target_index = window_size // 2\n",
    "    \n",
    "    words = collections.deque(data[window_size:])\n",
    "    window_words = collections.deque(data[:window_size], maxlen=window_size)\n",
    "    \n",
    "    for n in range(num_batches):\n",
    "        batch = np.ndarray(shape=(batch_size, window_size-1), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch[i,:] = context_window(window_words, target_index)\n",
    "            labels[i, 0] = window_words[target_index]\n",
    "            window_words.append(words.popleft())\n",
    "\n",
    "        yield batch, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
