{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso do word2vec para medir a similaridade entre termos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = 'wikipedia'\n",
    "DATA_DIR = os.path.join(HOME_DIR, 'data')\n",
    "\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "    \n",
    "TEXT_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "TEXT_FILENAME = TEXT_URL.split('/')[-1]\n",
    "TEXT_FILE = os.path.join(DATA_DIR, TEXT_FILENAME)\n",
    "\n",
    "text_missing = not os.path.isfile(TEXT_FILE)\n",
    "\n",
    "if text_missing:\n",
    "    print('Downloading {}...'.format(TEXT_FILENAME))\n",
    "    r = requests.get(TEXT_URL, stream=True)\n",
    "    with open(TEXT_FILE, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=32768):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti...\n",
      "\n",
      "(99,998,000 chars)\n",
      "\n",
      "...ousand defenders had set all the buildings but the food storerooms ablaze and committed mass suicide rather than face certain capture or defeat by their enemies because the jewish religion discourages the act of suicide however the defenders were reported to have drawn lots and slain each other in turn down to the last man who would be the only one to actually take his own life as per josephus account the argument is made that the storerooms were left standing to show that the defenders retained the ability to live and chose the time of their death this account of the siege of masada was apparently related to josephus by two women who survived the suicide by hiding inside a cistern along with five children and repeated elazar ben yair s final exortation to his followers prior to the mass suicide verbatim to the romans the site today platform access to the fortress the site of masada was identified in one eight four two and extensively excavated in one nine six three one nine six five b\n"
     ]
    }
   ],
   "source": [
    "def load_raw_text_from_zip(file):\n",
    "    with zipfile.ZipFile(file) as f:\n",
    "        return f.read(f.namelist()[0]).decode('utf-8')\n",
    "\n",
    "raw_text = load_raw_text_from_zip(TEXT_FILE)\n",
    "\n",
    "print('{}...\\n\\n({:,d} chars)\\n\\n...{}'.format(\n",
    "    raw_text[:1000], len(raw_text) - 2000, raw_text[-1000:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O vocabulário é formado a partir das palavras mais frequentes do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words (total):\n",
      "\n",
      "17,005,207\n",
      "\n",
      "Words (unique):\n",
      "\n",
      "253,854\n",
      "\n",
      "Most common:\n",
      "\n",
      "the (1,061,396)\n",
      "of (593,677)\n",
      "and (416,629)\n",
      "one (411,764)\n",
      "in (372,201)\n",
      "a (325,873)\n",
      "to (316,376)\n",
      "zero (264,975)\n",
      "nine (250,430)\n",
      "two (192,644)\n",
      "is (183,153)\n",
      "as (131,815)\n",
      "eight (125,285)\n",
      "for (118,445)\n",
      "s (116,710)\n",
      "five (115,789)\n",
      "three (114,775)\n",
      "was (112,807)\n",
      "by (111,831)\n",
      "that (109,510)\n",
      "\n",
      "Least common:\n",
      "\n",
      "triconodonts (1)\n",
      "katsumoto (1)\n",
      "spontainous (1)\n",
      "shpayder (1)\n",
      "operamusical (1)\n",
      "malbono (1)\n",
      "biomacla (1)\n",
      "cacher (1)\n",
      "wahlkapitulation (1)\n",
      "carousers (1)\n",
      "masn (1)\n",
      "pash (1)\n",
      "gwladgarwyr (1)\n",
      "insatiably (1)\n",
      "ldbp (1)\n",
      "higby (1)\n",
      "hemippus (1)\n",
      "ramzy (1)\n",
      "meserii (1)\n",
      "oogl (1)\n"
     ]
    }
   ],
   "source": [
    "words = raw_text.split()\n",
    "words_freq = collections.Counter(words).most_common()\n",
    "\n",
    "print('Words (total):\\n\\n{:,d}\\n'.format(len(words)))\n",
    "print('Words (unique):\\n\\n{:,d}\\n'.format(len(words_freq)))\n",
    "print('Most common:\\n')\n",
    "for word, freq in words_freq[:20]:\n",
    "    print('{} ({:,d})'.format(word, freq))\n",
    "print('\\nLeast common:\\n')\n",
    "for word, freq in words_freq[-20:]:\n",
    "    print('{} ({:,d})'.format(word, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, foram consideradas as palavras que com frequência acima de 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words 10+: 47,134\n"
     ]
    }
   ],
   "source": [
    "words_10plus = sum(1 for _, freq in words_freq if freq >= 10)\n",
    "\n",
    "print('Words 10+: {:,d}'.format(words_10plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('severance', 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = 47134\n",
    "\n",
    "words_freq[vocabulary_size - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words for the vocabulary: 47,133\n"
     ]
    }
   ],
   "source": [
    "words_vocab = words_freq[:(vocabulary_size-1)]\n",
    "\n",
    "print('Words for the vocabulary: {:,d}'.format(len(words_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 47134\n"
     ]
    }
   ],
   "source": [
    "UNK_ID = 0\n",
    "word_to_id = dict((word, word_id) for word_id, (word, _) in enumerate(words_vocab, UNK_ID+1))\n",
    "word_to_id['UNK'] = UNK_ID\n",
    "word_from_id = dict((word_id, word) for word, word_id in word_to_id.items())\n",
    "\n",
    "print('Vocabulary size: {:d}'.format(len(word_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK words: 206,721\n",
      "UNK frequency: 444,186\n"
     ]
    }
   ],
   "source": [
    "words_to_unk = words_freq[(vocabulary_size-1):]\n",
    "unk_freq = sum(freq for _, freq in words_to_unk)\n",
    "\n",
    "print('UNK words: {:,d}'.format(len(words_to_unk)))\n",
    "print('UNK frequency: {:,d}'.format(unk_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, o vocabulário é salvo em um arquivo .txt e este é carregado para definição de dicionários para as palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file size: 394,227 bytes\n"
     ]
    }
   ],
   "source": [
    "VOCABULARY_FILE = os.path.join(HOME_DIR, 'vocabulary.txt')\n",
    "\n",
    "with open(VOCABULARY_FILE, 'w') as f:\n",
    "    for word_id in range(vocabulary_size):\n",
    "        f.write(word_from_id[word_id] + '\\n')\n",
    "\n",
    "print('Vocabulary file size: {:,d} bytes'.format(os.stat(VOCABULARY_FILE).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 47,134\n"
     ]
    }
   ],
   "source": [
    "with open(VOCABULARY_FILE, newline='') as f:\n",
    "    word_from_id_ = dict((word_id, word.strip()) for word_id, word in enumerate(f))\n",
    "    word_to_id_ = dict((word, word_id) for word_id, word in word_from_id_.items())\n",
    "\n",
    "print('Vocabulary size: {:,d}'.format(len(word_to_id_)))\n",
    "assert word_to_id_ == word_to_id\n",
    "assert word_from_id_ == word_from_id\n",
    "del word_to_id_, word_from_id_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As palavras a partir de agora serão tratadas como índices que serão utilizados para os cálculos do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:\n",
      "\n",
      "17,005,207\n",
      "\n",
      "Text (IDs):\n",
      "\n",
      "[5242, 3082, 12, 6, 195, 2, 3137, 46, 59, 156]\n",
      "\n",
      "Text (Words):\n",
      "\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "data = list(word_to_id.get(word, UNK_ID) for word in words)\n",
    "\n",
    "print('Size:\\n\\n{:,d}\\n'.format(len(data)))\n",
    "print('Text (IDs):\\n\\n{}\\n'.format(data[:10]))\n",
    "print('Text (Words):\\n\\n{}'.format(list(word_from_id[word_id] for word_id in data[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável data representa o texto inicial codificado através dos índices que representam as palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até o momento o texto está representado por uma lista de índices. Aqui, será definido o dataset que será utilizado no\n",
    "treinamento do modelo. A variável resposta y é definida como um vetor de uma coluna e n linhas, onde o número de linhas corresponte ao\n",
    "número de palavras do vocabulário. Já as variáveis preditoras serão definidas pelo vetor X, onde o número de linhas\n",
    "também será o número de palavras do vocabulário. Já o número de colunas será o número de palavras no contexto.\n",
    "\n",
    "Um fato interessante sobre essa abordagem, é em relação ao número de colunas reduzido do dataset que quando comparado a abordagens de contagem, apresenta grande vantagem no custo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def context_window(window_words, target_index):\n",
    "    '''This function returns the words at the window without the target word.'''\n",
    "    words = list(window_words)\n",
    "    del words[target_index]\n",
    "    return words\n",
    "\n",
    "def input_cbow(data, batch_size, window_size):\n",
    "    '''This function goes through the data and creates input-output batches to train the model using gradient'''\n",
    "    if window_size % 2 == 0 or window_size < 3 \\\n",
    "        or window_size > (len(data) - batch_size) / 2:\n",
    "        # {window_size} must be odd: (n words left) target (n words right)\n",
    "        raise Exception(\n",
    "            'Invalid parameters: window_size must be a small odd number')\n",
    "\n",
    "    num_words = len(data)\n",
    "    num_windows = num_words - window_size + 1\n",
    "    num_batches = num_windows // batch_size\n",
    "    target_index = window_size // 2\n",
    "    \n",
    "    words = collections.deque(data[window_size:])\n",
    "    window_words = collections.deque(data[:window_size], maxlen=window_size)\n",
    "    \n",
    "    for n in range(num_batches):\n",
    "        batch = np.ndarray(shape=(batch_size, window_size-1), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            batch[i,:] = context_window(window_words, target_index)\n",
    "            labels[i, 0] = window_words[target_index]\n",
    "            window_words.append(words.popleft())\n",
    "\n",
    "        yield batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo implementado retorna uma matriz **embeddings** normalizada com dimensões (vocabulary_size, embedding_size), \n",
    "que representa as palavras como vetores em um espaço n-dimensional. O tamanho do espaço vetorial (embedding_size) \n",
    "é um hiperparâmetro que deve ser ajustado. É a partir dessa matriz que se torna possível medir a distância entre as\n",
    "palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cbow(vocabulary_size, embedding_size, num_sampled):\n",
    "    '''this function creates and returns all the cbow tensors needed.'''\n",
    "    X = tf.placeholder_with_default([[0]], shape=(None, None), name='X')\n",
    "    y = tf.placeholder_with_default([[0]], shape=(None, 1), name='y')\n",
    "\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform(shape=(vocabulary_size, embedding_size),\n",
    "                          minval=-1.0, maxval=1.0),\n",
    "        name='embeddings')\n",
    "    print('EMBEDDINGS: ')\n",
    "    print(embeddings)\n",
    "\n",
    "    X_embed = tf.nn.embedding_lookup(embeddings, X)\n",
    "    X_avg = tf.reduce_mean(X_embed, axis=1)\n",
    "\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(shape=(vocabulary_size, embedding_size),\n",
    "                            stddev=1.0 / np.sqrt(embedding_size)),\n",
    "        name='W')\n",
    "    softmax_biases = tf.Variable(\n",
    "        tf.zeros(shape=(vocabulary_size,)),\n",
    "        name='b')\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        sampled_loss = tf.nn.sampled_softmax_loss(weights=softmax_weights,\n",
    "                                                  biases=softmax_biases,\n",
    "                                                  inputs=X_avg,\n",
    "                                                  labels=y,\n",
    "                                                  num_sampled=num_sampled,\n",
    "                                                  num_classes=vocabulary_size)\n",
    "        loss = tf.reduce_mean(sampled_loss, name='mean')\n",
    "\n",
    "    norm = tf.norm(embeddings, axis=1, keep_dims=True)\n",
    "    normalized_embeddings = embeddings / norm\n",
    "\n",
    "    return X, y, normalized_embeddings, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def opt_adagrad(loss, learning_rate=1.0):\n",
    "    '''this function adds a gradient descent optimization called adagrad.'''\n",
    "    return tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_or_create_global_step(),\n",
    "        learning_rate=learning_rate,\n",
    "        optimizer='Adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model_fn, input_fn, opt_fn, query,\n",
    "          num_epochs=1, model_dir='/tmp/embedding_model', remove_model=True):\n",
    "    '''this function trains the model'''\n",
    "    if remove_model and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        X, y, embeddings, loss_op = model_fn()\n",
    "        train_op = opt_fn(loss_op)\n",
    "\n",
    "        query.build_graph(embeddings)\n",
    "\n",
    "        with tf.train.MonitoredTrainingSession(\n",
    "            checkpoint_dir=model_dir) as session:\n",
    "\n",
    "            for epoch in range(1, num_epochs+1):\n",
    "                print('Epoch {}\\n'.format(epoch))\n",
    "\n",
    "                avg_loss = 0\n",
    "                for step, (X_batch, y_batch) in enumerate(input_fn()):\n",
    "                    _, loss = session.run([train_op, loss_op],\n",
    "                                          feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "                    avg_loss = (loss + step * avg_loss) / (step + 1)\n",
    "                    if step % 10000 == 0:\n",
    "                        print('...{:,d} Average loss: {:.3f}'.format(\n",
    "                            step, avg_loss))\n",
    "\n",
    "                print('\\nAverage loss: {:.3f}\\n'.format(avg_loss))\n",
    "                query.run(session)\n",
    "                print()\n",
    "\n",
    "            return session.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_embeddings(file, embeddings):\n",
    "    '''This function saves the word embeddings' matrix into a file.'''\n",
    "    with open(file, 'w') as f:\n",
    "        vocabulary_size = embeddings.shape[0]\n",
    "        for word_id in range(vocabulary_size):\n",
    "            embedding = embeddings[word_id]\n",
    "            embedding_string = ('{:.5f}'.format(k) for k in embedding)\n",
    "            embedding_string = ' '.join(embedding_string)\n",
    "            f.write(embedding_string)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consulta de Palavras mais próximas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe **NearestNeibours** é utilizada para medir a distância entre os termos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NearestWordsQuery:\n",
    "\n",
    "    def __init__(self, word_from_id, words, k=4):\n",
    "        self.word_from_id = word_from_id\n",
    "        self.words = words\n",
    "        self.k = k\n",
    "\n",
    "    def build_graph(self, embeddings, name=None):\n",
    "        with tf.name_scope(name, \"nearest_words\", [self.words, self.k]):\n",
    "            input_words = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "            input_embed = tf.nn.embedding_lookup(embeddings, input_words)\n",
    "            similarity = tf.matmul(input_embed, embeddings, transpose_b=True)\n",
    "            nearest = tf.nn.top_k(similarity, self.k+1)\n",
    "\n",
    "        self.input_words = {input_words: self.words}\n",
    "        self.nearest = nearest\n",
    "\n",
    "    def nearest_words(self, target_id, nearest_indices, nearest_values):\n",
    "        id_pairs = zip(nearest_indices, nearest_values)\n",
    "        word_pairs = list((self.word_from_id[word_id], value)\n",
    "                          for word_id, value in id_pairs\n",
    "                          if word_id != target_id)\n",
    "        return word_pairs[:self.k]\n",
    "\n",
    "    def format_words(self, word_pairs):\n",
    "        return ('{} ({:,.3f})'.format(word, value)\n",
    "                for word, value in word_pairs)\n",
    "\n",
    "    def run(self, session):\n",
    "        nearest_val, nearest_id = session.run(self.nearest,\n",
    "                                              feed_dict=self.input_words)\n",
    "        for i, word_id in enumerate(self.words):\n",
    "            word = self.word_from_id[word_id]\n",
    "            nearest_words = self.nearest_words(\n",
    "                word_id, nearest_id[i], nearest_val[i])\n",
    "            nearest_words = ', '.join(self.format_words(nearest_words))\n",
    "            print('{}: {}'.format(word, nearest_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O experimento a princípio é realizado a partir do treinamento do modelo CBOW com dados da wikipedia, onde a avaliação\n",
    "é realizada a partir da média da perda (average loss) a medida que o modelo evolui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para se ter uma ‘percepção qualitativa’ do resultado, são amostradas 8 palavras do intervalo das 1000 mais comuns - no final de cada época, essa amostra é usada para gerar a lista de similaridade. Essa lista pode ser observada para ver como o aprendizado evolui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43387\n",
      "936\n",
      "166\n",
      "71\n",
      "2170\n"
     ]
    }
   ],
   "source": [
    "# valid_num_words = 8\n",
    "# valid_range_words = 1000\n",
    "# valid_words = random.sample(range(1, valid_range_words), valid_num_words)\n",
    "\n",
    "words_to_test = ['ronaldo', 'jesus', 'music', 'world', 'computing']\n",
    "valid_words = [word_to_id[word] for word in words_to_test]\n",
    "print(valid_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A amostra de palavras é encapsulada no objeto **nearest_words** que consulta palavras similares a partir da representação vetorial, é sendo usada no treinamento para comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_words = NearestWordsQuery(word_from_id, valid_words, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDINGS: \n",
      "<tf.Variable 'embeddings:0' shape=(47134, 128) dtype=float32_ref>\n",
      "Epoch 1\n",
      "\n",
      "...0 Average loss: 7.854\n",
      "...10,000 Average loss: 3.436\n",
      "...20,000 Average loss: 3.266\n",
      "...30,000 Average loss: 3.181\n",
      "...40,000 Average loss: 3.114\n",
      "...50,000 Average loss: 3.072\n",
      "...60,000 Average loss: 3.031\n",
      "...70,000 Average loss: 2.995\n",
      "...80,000 Average loss: 2.966\n",
      "...90,000 Average loss: 2.941\n",
      "...100,000 Average loss: 2.913\n",
      "...110,000 Average loss: 2.884\n",
      "...120,000 Average loss: 2.867\n",
      "...130,000 Average loss: 2.845\n",
      "\n",
      "Average loss: 2.842\n",
      "\n",
      "chemical: physical (0.413), simplest (0.384), crude (0.369), mathematical (0.352)\n",
      "royal: yunnan (0.361), auld (0.337), alma (0.334), porvoo (0.327)\n",
      "video: digital (0.390), cooh (0.383), computer (0.373), radio (0.350)\n",
      "paul: thomas (0.452), peter (0.445), charles (0.422), james (0.393)\n",
      "alexander: frederick (0.373), leo (0.360), boltzmann (0.349), clement (0.347)\n",
      "that: which (0.525), however (0.473), what (0.398), permits (0.371)\n",
      "an: rankin (0.339), classless (0.335), serbians (0.329), distract (0.328)\n",
      "version: versions (0.464), edition (0.445), decade (0.379), feature (0.366)\n",
      "\n",
      "CPU times: user 17min 14s, sys: 1min 16s, total: 18min 31s\n",
      "Wall time: 11min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = os.path.join('word2vec', 'cbow')\n",
    "EMBEDDINGS_FILE = os.path.join('word2vec', 'cbow.txt')\n",
    "\n",
    "vocabulary_size = len(word_to_id)\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n",
    "batch_size = 128\n",
    "window_size = 3\n",
    "\n",
    "model_fn = lambda: model_cbow(vocabulary_size, embedding_size, num_sampled)\n",
    "input_fn = lambda: input_cbow(data, batch_size, window_size)\n",
    "opt_fn = lambda loss: opt_adagrad(loss, learning_rate=1.0)\n",
    "\n",
    "cbow_embeddings = train(model_fn,\n",
    "                        input_fn,\n",
    "                        opt_fn,\n",
    "                        nearest_words,\n",
    "                        num_epochs=1,\n",
    "                        model_dir=MODEL_DIR)\n",
    "\n",
    "save_embeddings(EMBEDDINGS_FILE, cbow_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir dos resultados acima, é possível observar que o **average loss** decresce lentamente se comparado ao número de passos (130 mil). Os exemplos observados parecem fazer sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mais um exemplo: Treinamento com dados de Inquéritos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o propósito do estudo é utilizar o word2vec em documentos de inquéritos policiais, foi realizado o treinamento de um modelo com resumos de Inquéritos Policiais e foi observado os resultados a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/casos.csv\")\n",
    "summaries = \"\".join(l for l in df['DS_RESUMO'].str.cat(sep=', ') if l not in string.punctuation)\n",
    "words = summaries.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_freq = collections.Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_3plus = sum(1 for _, freq in words_freq if freq >= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 4131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_vocab = words_freq[:(vocabulary_size-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNK_ID = 0\n",
    "word_to_id = dict((word, word_id) for word_id, (word, _) in enumerate(words_vocab, UNK_ID+1))\n",
    "word_to_id['UNK'] = UNK_ID\n",
    "word_from_id = dict((word_id, word) for word, word_id in word_to_id.items())\n",
    "\n",
    "print('Vocabulary size: {:d}'.format(len(word_to_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_to_unk = words_freq[(vocabulary_size-1):]\n",
    "unk_freq = sum(freq for _, freq in words_to_unk)\n",
    "\n",
    "print('UNK words: {:,d}'.format(len(words_to_unk)))\n",
    "print('UNK frequency: {:,d}'.format(unk_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCABULARY_FILE = '../data/vocabulary.txt'\n",
    "\n",
    "with open(VOCABULARY_FILE, 'w') as f:\n",
    "    for word_id in range(vocabulary_size):\n",
    "        f.write(word_from_id[word_id] + '\\n')\n",
    "\n",
    "print('Vocabulary file size: {:,d} bytes'.format(os.stat(VOCABULARY_FILE).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(VOCABULARY_FILE, newline='') as f:\n",
    "    word_from_id_ = dict((word_id, word.strip()) for word_id, word in enumerate(f))\n",
    "    word_to_id_ = dict((word, word_id) for word_id, word in word_from_id_.items())\n",
    "\n",
    "# print(word_from_id_)\n",
    "print('Vocabulary size: {:,d}'.format(len(word_to_id_)))\n",
    "assert word_to_id_ == word_to_id\n",
    "assert word_from_id_ == word_from_id\n",
    "del word_to_id_, word_from_id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = list(word_to_id.get(word, UNK_ID) for word in words)\n",
    "\n",
    "print('Size:\\n\\n{:,d}\\n'.format(len(data)))\n",
    "print('Text (IDs):\\n\\n{}\\n'.format(data[:10]))\n",
    "print('Text (Words):\\n\\n{}'.format(list(word_from_id[word_id] for word_id in data[:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_num_words = 8\n",
    "valid_range_words = 1000\n",
    "valid_words = random.sample(range(1, valid_range_words), valid_num_words)\n",
    "\n",
    "for word_id in valid_words:\n",
    "    print(word_from_id[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_words = NearestWordsQuery(word_from_id, valid_words, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL_DIR = os.path.join('word2vec', 'cbow')\n",
    "EMBEDDINGS_FILE = os.path.join('word2vec', 'cbow.txt')\n",
    "\n",
    "vocabulary_size = len(word_to_id)\n",
    "embedding_size = 128\n",
    "num_sampled = 64\n",
    "\n",
    "batch_size = 256\n",
    "window_size = 3\n",
    "\n",
    "model_fn = lambda: model_cbow(vocabulary_size, embedding_size, num_sampled)\n",
    "input_fn = lambda: input_cbow(data, batch_size, window_size)\n",
    "opt_fn = lambda loss: opt_adagrad(loss, learning_rate=1.0)\n",
    "\n",
    "cbow_embeddings = train(model_fn,\n",
    "                        input_fn,\n",
    "                        opt_fn,\n",
    "                        nearest_words,\n",
    "                        num_epochs=1,\n",
    "                        model_dir=MODEL_DIR)\n",
    "\n",
    "save_embeddings(EMBEDDINGS_FILE, cbow_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
